#!/usr/bin/env python3
"""
å¼‚æ­¥æ€§èƒ½å¯¹æ¯”æµ‹è¯•è„šæœ¬

å¿«é€Ÿæµ‹è¯•åŒæ­¥ vs å¼‚æ­¥æ¨¡å¼çš„æ€§èƒ½å·®å¼‚
"""

import asyncio
import time
import sys
import logging
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from phase1_embedding.models.xinference_client import XinferenceClient
from phase1_embedding.models.async_xinference_client import AsyncXinferenceClient
from phase1_embedding.data.dataset_loader import DatasetLoader

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def save_result(result: dict, mode: str, model: str, num_docs: int, batch_size: int, concurrent: int = None):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ° JSON æ–‡ä»¶
    
    Args:
        result: æµ‹è¯•ç»“æœå­—å…¸
        mode: æµ‹è¯•æ¨¡å¼ (sync/async)
        model: æ¨¡å‹åç§°
        num_docs: æ–‡æ¡£æ•°é‡
        batch_size: æ‰¹æ¬¡å¤§å°
        concurrent: å¹¶å‘æ•°ï¼ˆä»…å¼‚æ­¥æ¨¡å¼ï¼‰
    """
    # åˆ›å»ºç»“æœç›®å½•
    results_dir = Path("quick_test_results")
    results_dir.mkdir(exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶åï¼šmode_timestamp.json
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{mode}_{timestamp}.json"
    filepath = results_dir / filename
    
    # æ·»åŠ å…ƒæ•°æ®
    full_result = {
        "test_info": {
            "mode": mode,
            "model": model,
            "num_docs": num_docs,
            "batch_size": batch_size,
            "concurrent_requests": concurrent if mode == "async" else None,
            "timestamp": timestamp,
            "datetime": datetime.now().isoformat()
        },
        "results": result
    }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(full_result, f, indent=2, ensure_ascii=False)
    
    logger.info(f"âœ“ Results saved to: {filepath}")
    return filepath


def test_sync_mode(client: XinferenceClient, model: str, texts: list, batch_size: int):
    """æµ‹è¯•åŒæ­¥æ¨¡å¼æ€§èƒ½"""
    logger.info(f"\n{'='*60}")
    logger.info(f"SYNC MODE TEST")
    logger.info(f"{'='*60}")
    logger.info(f"Model: {model}")
    logger.info(f"Texts: {len(texts)}")
    logger.info(f"Batch size: {batch_size}")
    
    start_time = time.time()
    
    # åˆ†æ‰¹å¤„ç†
    total_vectors = 0
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        embeddings = client.embed_batch(batch, model, batch_size)
        if embeddings is not None:
            total_vectors += len(embeddings)
    
    elapsed = time.time() - start_time
    throughput = total_vectors / elapsed
    
    logger.info(f"\nâœ“ SYNC Results:")
    logger.info(f"  Vectors generated: {total_vectors}")
    logger.info(f"  Time: {elapsed:.2f}s")
    logger.info(f"  Throughput: {throughput:.2f} docs/s")
    
    return {
        "mode": "sync",
        "vectors": total_vectors,
        "time": elapsed,
        "throughput": throughput
    }


async def test_async_mode(
    client: AsyncXinferenceClient,
    model: str,
    texts: list,
    batch_size: int
):
    """æµ‹è¯•å¼‚æ­¥æ¨¡å¼æ€§èƒ½"""
    logger.info(f"\n{'='*60}")
    logger.info(f"ASYNC MODE TEST")
    logger.info(f"{'='*60}")
    logger.info(f"Model: {model}")
    logger.info(f"Texts: {len(texts)}")
    logger.info(f"Batch size: {batch_size}")
    logger.info(f"Concurrent requests: {client.max_concurrent_requests}")
    
    start_time = time.time()
    
    # å¹¶å‘å¤„ç†
    embeddings = await client.embed_concurrent(
        texts,
        model,
        batch_size=batch_size,
        show_progress=True
    )
    
    elapsed = time.time() - start_time
    total_vectors = len(embeddings) if embeddings is not None else 0
    throughput = total_vectors / elapsed if elapsed > 0 else 0
    
    logger.info(f"\nâœ“ ASYNC Results:")
    logger.info(f"  Vectors generated: {total_vectors}")
    logger.info(f"  Time: {elapsed:.2f}s")
    logger.info(f"  Throughput: {throughput:.2f} docs/s")
    
    return {
        "mode": "async",
        "vectors": total_vectors,
        "time": elapsed,
        "throughput": throughput
    }


async def main(test_mode="both", num_docs=1000, batch_size=128, concurrent=8):
    """ä¸»æµ‹è¯•å‡½æ•°
    
    Args:
        test_mode: æµ‹è¯•æ¨¡å¼ - "sync", "async", "both"
        num_docs: æµ‹è¯•æ–‡æ¡£æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        concurrent: å¹¶å‘è¯·æ±‚æ•°ï¼ˆä»…å¼‚æ­¥æ¨¡å¼ï¼‰
    """
    logger.info("="*80)
    logger.info(f"Phase 1 æ€§èƒ½æµ‹è¯• - æ¨¡å¼: {test_mode.upper()}")
    logger.info("="*80)
    
    # é…ç½®
    host = "192.168.1.51"
    port = 9997
    num_test_docs = num_docs
    concurrent_requests = concurrent
    
    try:
        # 1. åŠ è½½æµ‹è¯•æ•°æ®
        logger.info("\n1. Loading test data...")
        loader = DatasetLoader(data_dir="../datasets/processed")
        
        if not loader.check_dataset():
            logger.error("Dataset not found! Please prepare dataset first:")
            logger.error("  cd datasets/scripts && ./quick_start.sh 100000")
            return 1
        
        documents = loader.sample_documents(num_samples=num_test_docs, seed=42)
        texts = [doc["text"] for doc in documents]
        logger.info(f"âœ“ Loaded {len(texts)} test documents")
        
        # 2. æ£€æŸ¥å¯ç”¨æ¨¡å‹
        logger.info("\n2. Checking available models...")
        sync_client = XinferenceClient(host=host, port=port)
        
        if not sync_client.check_health():
            logger.error("Xinference service is not available!")
            return 1
        
        available_models = sync_client.get_available_model_ids()
        if not available_models:
            logger.error("No models available!")
            return 1
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªæ¨¡å‹è¿›è¡Œæµ‹è¯•
        test_model = available_models[0]
        logger.info(f"âœ“ Using model: {test_model}")
        
        sync_results = None
        async_results = None
        
        # 3. æ ¹æ®æ¨¡å¼è¿è¡Œæµ‹è¯•
        if test_mode in ["sync", "both"]:
            logger.info(f"\n3. Testing SYNC mode...")
            sync_results = test_sync_mode(sync_client, test_model, texts, batch_size)
            
            # ä¿å­˜åŒæ­¥ç»“æœ
            save_result(sync_results, "sync", test_model, num_test_docs, batch_size)
        
        if test_mode in ["async", "both"]:
            step = 4 if test_mode == "both" else 3
            logger.info(f"\n{step}. Testing ASYNC mode...")
            async with AsyncXinferenceClient(
                host=host,
                port=port,
                max_concurrent_requests=concurrent_requests
            ) as async_client:
                async_results = await test_async_mode(
                    async_client,
                    test_model,
                    texts,
                    batch_size
                )
            
            # ä¿å­˜å¼‚æ­¥ç»“æœ
            save_result(async_results, "async", test_model, num_test_docs, batch_size, concurrent_requests)
        
        # 5. å¯¹æ¯”ç»“æœï¼ˆä»…åœ¨ both æ¨¡å¼ä¸‹ï¼‰
        if test_mode == "both" and sync_results and async_results:
            logger.info("\n" + "="*80)
            logger.info("PERFORMANCE COMPARISON")
            logger.info("="*80)
            
            speedup = async_results["throughput"] / sync_results["throughput"]
            time_saved = sync_results["time"] - async_results["time"]
            time_saved_percent = (time_saved / sync_results["time"]) * 100
            
            logger.info(f"\nSync Mode:")
            logger.info(f"  Time: {sync_results['time']:.2f}s")
            logger.info(f"  Throughput: {sync_results['throughput']:.2f} docs/s")
            
            logger.info(f"\nAsync Mode:")
            logger.info(f"  Time: {async_results['time']:.2f}s")
            logger.info(f"  Throughput: {async_results['throughput']:.2f} docs/s")
            
            logger.info(f"\nğŸš€ Performance Gain:")
            logger.info(f"  Speedup: {speedup:.2f}x")
            logger.info(f"  Time saved: {time_saved:.2f}s ({time_saved_percent:.1f}%)")
            logger.info(f"  Estimated time for 3M vectors:")
            logger.info(f"    Sync:  {(3000000 / sync_results['throughput']) / 3600:.2f} hours")
            logger.info(f"    Async: {(3000000 / async_results['throughput']) / 3600:.2f} hours")
            
            # ä¿å­˜å¯¹æ¯”ç»“æœ
            comparison = {
                "sync": sync_results,
                "async": async_results,
                "comparison": {
                    "speedup": speedup,
                    "time_saved_seconds": time_saved,
                    "time_saved_percent": time_saved_percent
                }
            }
            save_result(comparison, "comparison", test_model, num_test_docs, batch_size, concurrent_requests)
            
            # å»ºè®®
            logger.info("\n" + "="*80)
            logger.info("RECOMMENDATIONS")
            logger.info("="*80)
            
            if speedup >= 3.0:
                logger.info("âœ“ å¼‚æ­¥æ¨¡å¼æ€§èƒ½æå‡æ˜¾è‘—ï¼å¼ºçƒˆå»ºè®®ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ã€‚")
            elif speedup >= 2.0:
                logger.info("âœ“ å¼‚æ­¥æ¨¡å¼æœ‰æ˜æ˜¾æ€§èƒ½æå‡ï¼Œæ¨èä½¿ç”¨ã€‚")
            elif speedup >= 1.5:
                logger.info("âœ“ å¼‚æ­¥æ¨¡å¼æœ‰ä¸€å®šæ€§èƒ½æå‡ã€‚")
            else:
                logger.info("âš  å¼‚æ­¥æ¨¡å¼æå‡æœ‰é™ï¼Œå¯èƒ½å­˜åœ¨ç“¶é¢ˆï¼š")
                logger.info("  - æ£€æŸ¥ Xinference æœåŠ¡ç«¯é…ç½®")
                logger.info("  - æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿ")
                logger.info("  - å°è¯•å¢å¤§ concurrent_requests")
        
        # 6. ä¸‹ä¸€æ­¥å»ºè®®
        logger.info(f"\n{'='*80}")
        logger.info("NEXT STEPS")
        logger.info("="*80)
        logger.info(f"  æŸ¥çœ‹ä¿å­˜çš„ç»“æœ: quick_test_results/")
        logger.info(f"  è¿è¡Œå®Œæ•´æµ‹è¯•: python run_phase1.py --async --batch 1")
        logger.info(f"  æ±‡æ€»å¤šæ¬¡ç»“æœ: python compare_results.py")
        
        return 0
        
    except Exception as e:
        logger.error(f"\nâœ— Test failed: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Phase 1 æ€§èƒ½æµ‹è¯•",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä»…æµ‹è¯•åŒæ­¥æ¨¡å¼
  python test_async_performance.py --mode sync
  
  # ä»…æµ‹è¯•å¼‚æ­¥æ¨¡å¼
  python test_async_performance.py --mode async
  
  # åŒæ—¶æµ‹è¯•å¹¶å¯¹æ¯”
  python test_async_performance.py --mode both
  
  # è‡ªå®šä¹‰å‚æ•°
  python test_async_performance.py --mode async --num-docs 5000 --concurrent 16
        """
    )
    parser.add_argument(
        "--mode",
        type=str,
        choices=["sync", "async", "both"],
        default="both",
        help="æµ‹è¯•æ¨¡å¼: sync(åŒæ­¥), async(å¼‚æ­¥), both(å¯¹æ¯”)"
    )
    parser.add_argument("--num-docs", type=int, default=1000, help="æµ‹è¯•æ–‡æ¡£æ•°é‡")
    parser.add_argument("--batch-size", type=int, default=128, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--concurrent", type=int, default=8, help="å¹¶å‘è¯·æ±‚æ•°ï¼ˆå¼‚æ­¥æ¨¡å¼ï¼‰")
    
    args = parser.parse_args()
    
    exit_code = asyncio.run(main(
        test_mode=args.mode,
        num_docs=args.num_docs,
        batch_size=args.batch_size,
        concurrent=args.concurrent
    ))
    sys.exit(exit_code)
