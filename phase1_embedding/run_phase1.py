#!/usr/bin/env python3
"""
Phase 1: å‘é‡ç”Ÿæˆæ€§èƒ½æµ‹è¯• - ä¸»ç¨‹åº

ä¸²è¡Œæµ‹è¯•4ä¸ªåµŒå…¥æ¨¡å‹çš„æ¨ç†æ€§èƒ½å¹¶ç”Ÿæˆ300ä¸‡å‘é‡ç¼“å­˜
æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥ä¸¤ç§æ¨¡å¼
"""

import argparse
import asyncio
import logging
import sys
import yaml
from pathlib import Path
from datetime import datetime

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from phase1_embedding.models.xinference_client import XinferenceClient
from phase1_embedding.models.async_xinference_client import AsyncXinferenceClient
from phase1_embedding.data.dataset_loader import DatasetLoader
from phase1_embedding.benchmarks.inference_benchmark import InferenceBenchmark
from phase1_embedding.benchmarks.async_inference_benchmark import AsyncInferenceBenchmark
from phase1_embedding.report_generator import Phase1ReportGenerator


def setup_logging(log_dir: str, log_file: str, console_level: str = "INFO"):
    """
    é…ç½®æ—¥å¿—
    
    Args:
        log_dir: æ—¥å¿—ç›®å½•
        log_file: æ—¥å¿—æ–‡ä»¶å
        console_level: æ§åˆ¶å°æ—¥å¿—çº§åˆ«
    """
    log_dir_path = Path(log_dir)
    log_dir_path.mkdir(parents=True, exist_ok=True)
    
    # æ–‡ä»¶æ—¥å¿—
    log_path = log_dir_path / log_file
    file_handler = logging.FileHandler(log_path, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(file_formatter)
    
    # æ§åˆ¶å°æ—¥å¿—
    console_handler = logging.StreamHandler()
    console_handler.setLevel(getattr(logging, console_level.upper()))
    console_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )
    console_handler.setFormatter(console_formatter)
    
    # é…ç½®root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    
    logging.info(f"Logging initialized: {log_path}")


def load_config(config_file: str) -> dict:
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        é…ç½®å­—å…¸
    """
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


async def main_async(args, config, logger, validated_models, documents, test_texts):
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    xinference_config = config["xinference"]
    async_config = config.get("async_inference", {})
    report_config = config.get("report", {})
    cache_config = config.get("vector_cache", {})
    serial_config = config.get("serial_execution", {})
    
    # åº”ç”¨é¢„è®¾ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.async_preset and "presets" in async_config:
        preset = async_config["presets"].get(args.async_preset)
        if preset:
            logger.info(f"Applying async preset: {args.async_preset}")
            async_config.update(preset)
    
    # åˆå§‹åŒ–å¼‚æ­¥å®¢æˆ·ç«¯
    concurrent_requests = async_config.get("concurrent_requests", 8)
    connection_pool_size = async_config.get("connection_pool_size", 32)
    
    logger.info(f"\nInitializing async Xinference client...")
    logger.info(f"  Concurrent requests: {concurrent_requests}")
    logger.info(f"  Connection pool size: {connection_pool_size}")
    
    async with AsyncXinferenceClient(
        host=xinference_config["host"],
        port=xinference_config["port"],
        timeout=xinference_config.get("timeout", 300),
        max_concurrent_requests=concurrent_requests,
        connection_pool_size=connection_pool_size
    ) as async_client:
        
        if not await async_client.check_health():
            raise RuntimeError("Xinference service is not available")
        
        logger.info("âœ“ Async Xinference client connected")
        
        # åˆå§‹åŒ–å¼‚æ­¥åŸºå‡†æµ‹è¯•
        benchmark = AsyncInferenceBenchmark(
            async_client=async_client,
            output_dir=report_config.get("output_dir", "phase1_results")
        )
        
        # è¿è¡Œå¼‚æ­¥åŸºå‡†æµ‹è¯•
        logger.info(f"\nStarting async serial benchmark...")
        logger.info(f"  Concurrent requests: {concurrent_requests}")
        logger.info(f"  Auto batch tuning: {async_config.get('auto_batch_tuning', True)}")
        logger.info(f"  Pause between models: {serial_config.get('pause_between_models', 5)}s")
        
        await benchmark.run_serial_benchmark_async(
            models=validated_models,
            test_texts=test_texts,
            documents=documents,
            cache_dir=cache_config.get("output_dir", "vector_cache"),
            auto_tune_batch_size=async_config.get("auto_batch_tuning", True),
            pause_between_models=serial_config.get("pause_between_models", 5)
        )
        
        # ä¿å­˜ç»“æœ
        logger.info(f"\nSaving async results...")
        benchmark.save_results()
        
        # æ‰“å°æ‘˜è¦
        logger.info("\n" + "="*80)
        logger.info("ASYNC BENCHMARK SUMMARY")
        logger.info("="*80)
        
        summary = benchmark.get_summary()
        for model_summary in summary["models"]:
            logger.info(f"\n{model_summary['name']}:")
            logger.info(f"  Throughput: {model_summary['throughput_docs_per_sec']:.2f} docs/s")
            logger.info(f"  Optimal batch size: {model_summary['optimal_batch_size']}")
            logger.info(f"  Concurrent requests: {model_summary['concurrent_requests']}")
            logger.info(f"  GPU peak memory: {model_summary['gpu_peak_memory_mb']:.2f} MB")
            logger.info(f"  Time for 3M vectors: {model_summary['time_for_3m_vectors_hours']:.2f} hours")
            if model_summary.get('speedup_factor'):
                logger.info(f"  Speedup vs sync: {model_summary['speedup_factor']:.2f}x")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Phase 1: å‘é‡ç”Ÿæˆæ€§èƒ½æµ‹è¯•ï¼ˆæ”¯æŒåŒæ­¥/å¼‚æ­¥æ¨¡å¼ï¼‰"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="../config/phase1_config.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--async",
        dest="async_mode",
        action="store_true",
        help="å¯ç”¨å¼‚æ­¥å¹¶å‘æ¨¡å¼ï¼ˆæ˜¾è‘—æå‡GPUåˆ©ç”¨ç‡å’Œååé‡ï¼‰"
    )
    parser.add_argument(
        "--async-preset",
        type=str,
        choices=["conservative", "balanced", "aggressive"],
        help="å¼‚æ­¥æ¨¡å¼é¢„è®¾ï¼šconservative(ç¨³å®š), balanced(æ¨è), aggressive(æé™æ€§èƒ½)"
    )
    parser.add_argument(
        "--serial",
        action="store_true",
        default=True,
        help="ä¸²è¡Œæ‰§è¡Œï¼ˆå¼ºåˆ¶ï¼‰"
    )
    parser.add_argument(
        "--models",
        type=str,
        nargs="+",
        help="æŒ‡å®šè¦æµ‹è¯•çš„æ¨¡å‹ï¼ˆé»˜è®¤æµ‹è¯•æ‰€æœ‰ï¼‰"
    )
    parser.add_argument(
        "--batch",
        type=int,
        help="æŒ‡å®šè¦è¿è¡Œçš„æ‰¹æ¬¡IDï¼ˆå¦‚æœé…ç½®æ–‡ä»¶ä¸­å®šä¹‰äº†batch_groupsï¼‰"
    )
    parser.add_argument(
        "--list-batches",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ‰¹æ¬¡å¹¶é€€å‡º"
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    print(f"Loading config from {args.config}")
    config = load_config(args.config)
    
    # è®¾ç½®æ—¥å¿—
    logging_config = config.get("logging", {})
    setup_logging(
        log_dir=logging_config.get("log_dir", "logs"),
        log_file=logging_config.get("log_file", "phase1.log"),
        console_level=logging_config.get("level", "INFO")
    )
    
    logger = logging.getLogger(__name__)
    
    logger.info("="*80)
    logger.info("Phase 1: å‘é‡ç”Ÿæˆæ€§èƒ½æµ‹è¯•")
    logger.info(f"Mode: {'ASYNC (High Performance)' if args.async_mode else 'SYNC (Standard)'}")
    logger.info("="*80)
    logger.info(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # å¦‚æœæŒ‡å®šäº† --list-batchesï¼Œåˆ—å‡ºæ‰€æœ‰æ‰¹æ¬¡å¹¶é€€å‡ºï¼ˆä¸éœ€è¦è¿æ¥Xinferenceï¼‰
    if args.list_batches:
        batch_groups = config.get("batch_groups", [])
        if not batch_groups:
            print("No batch_groups defined in config file")
            return 0
        
        print("\nAvailable batches:")
        for batch in batch_groups:
            batch_id = batch.get("batch_id", "?")
            batch_name = batch.get("batch_name", "unnamed")
            model_names = batch.get("model_names", [])
            print(f"\n  Batch {batch_id}: {batch_name}")
            print(f"    Models: {', '.join(model_names)}")
            print(f"    Run with: python run_phase1.py --config {args.config} --batch {batch_id}")
        return 0
    
    try:
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å®‰è£…ä¾èµ–
        if args.async_mode:
            try:
                import httpx
            except ImportError:
                logger.error("Async mode requires 'httpx' package. Install it with: pip install httpx")
                return 1
            
            try:
                from tqdm.asyncio import tqdm as async_tqdm
            except ImportError:
                logger.warning("'tqdm' asyncio support not found. Install with: pip install tqdm")
        
        # 1. åˆå§‹åŒ–Xinferenceå®¢æˆ·ç«¯ï¼ˆä»…ç”¨äºæ¨¡å‹éªŒè¯ï¼‰
        xinference_config = config["xinference"]
        logger.info(f"\nConnecting to Xinference at {xinference_config['host']}:{xinference_config['port']}")
        
        client = XinferenceClient(
            host=xinference_config["host"],
            port=xinference_config["port"],
            timeout=xinference_config.get("timeout", 300)
        )
        
        if not client.check_health():
            raise RuntimeError("Xinference service is not available")
        
        logger.info("âœ“ Xinference client connected")
        
        # 1.5. å¤„ç†æ‰¹æ¬¡é…ç½®
        all_models = config["models"]
        models_to_test = all_models
        
        # å¦‚æœæŒ‡å®šäº†æ‰¹æ¬¡ï¼Œåˆ™è¿‡æ»¤æ¨¡å‹
        if args.batch is not None:
            batch_groups = config.get("batch_groups", [])
            if not batch_groups:
                logger.error("--batch specified but no batch_groups defined in config file")
                return 1
            
            # æŸ¥æ‰¾æŒ‡å®šçš„æ‰¹æ¬¡
            selected_batch = None
            for batch in batch_groups:
                if batch.get("batch_id") == args.batch:
                    selected_batch = batch
                    break
            
            if not selected_batch:
                logger.error(f"Batch {args.batch} not found in config. Available batches:")
                for batch in batch_groups:
                    logger.error(f"  Batch {batch.get('batch_id')}: {batch.get('batch_name', 'unnamed')}")
                return 1
            
            batch_model_names = set(selected_batch.get("model_names", []))
            models_to_test = [m for m in all_models if m["name"] in batch_model_names]
            
            if not models_to_test:
                logger.error(f"No models found in batch {args.batch}")
                return 1
            
            logger.info(f"\nğŸ“¦ Running batch {args.batch}: {selected_batch.get('batch_name', 'unnamed')}")
            logger.info(f"  Models in this batch: {', '.join([m['name'] for m in models_to_test])}")
        
        # å¦‚æœæŒ‡å®šäº† --modelsï¼Œåˆ™è¿›ä¸€æ­¥è¿‡æ»¤
        if args.models:
            models_to_test = [m for m in models_to_test if m["name"] in args.models]
            if not models_to_test:
                logger.error(f"No matching models found: {args.models}")
                return 1
        
        # 1.6. éªŒè¯æ‰€æœ‰æ¨¡å‹æ˜¯å¦å­˜åœ¨
        logger.info("\nValidating models...")
        
        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        available_models = client.get_available_model_ids()
        logger.info(f"Available models on Xinference ({len(available_models)}):")
        for m in available_models[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            logger.info(f"  - {m}")
        if len(available_models) > 10:
            logger.info(f"  ... and {len(available_models) - 10} more")
        
        # éªŒè¯æ¯ä¸ªæ¨¡å‹
        validated_models = []
        for model_config in models_to_test:
            model_name = model_config["name"]
            model_full_name = model_config["model_name"]
            
            exists, actual_id = client.check_model_exists(model_full_name)
            if not exists:
                logger.error(
                    f"\nâœ— Model '{model_full_name}' (config name: '{model_name}') not found!\n"
                    f"  Please ensure the model is loaded in Xinference.\n"
                    f"  You can check available models with: curl http://{xinference_config['host']}:{xinference_config['port']}/v1/models"
                )
                # ç»§ç»­éªŒè¯å…¶ä»–æ¨¡å‹ï¼Œä½†è®°å½•é”™è¯¯
            else:
                if actual_id and actual_id != model_full_name:
                    logger.warning(
                        f"âš  Model name mismatch for '{model_name}':\n"
                        f"  Config uses: '{model_full_name}'\n"
                        f"  Xinference has: '{actual_id}'\n"
                        f"  Will use: '{actual_id}'"
                    )
                    # æ›´æ–°é…ç½®ä¸­çš„æ¨¡å‹åç§°
                    model_config["model_name"] = actual_id
                validated_models.append(model_config)
                logger.info(f"âœ“ Model '{model_name}' validated: {actual_id or model_full_name}")
        
        if not validated_models:
            logger.error("\nâœ— No valid models found! Please check your configuration and Xinference setup.")
            return 1
        
        logger.info(f"\nâœ“ Validated {len(validated_models)}/{len(models_to_test)} models")
        
        # 2. å‡†å¤‡æ•°æ®é›†
        dataset_config = config["dataset"]
        logger.info(f"\nPreparing dataset: {dataset_config['name']}")
        logger.info(f"  Target size: {dataset_config['sample_size']} documents")
        
        loader = DatasetLoader(data_dir=dataset_config["path"])
        
        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
        if not loader.check_dataset():
            raise RuntimeError(
                "Dataset not found. Please prepare dataset first:\n"
                "  cd datasets/scripts && ./quick_start.sh 100000"
            )
        
        # é‡‡æ ·æ–‡æ¡£
        documents = loader.sample_documents(
            num_samples=dataset_config["sample_size"],
            seed=dataset_config.get("seed", 42)
        )
        
        logger.info(f"âœ“ Dataset prepared: {len(documents)} documents")
        
        # å‡†å¤‡æµ‹è¯•æ–‡æœ¬ï¼ˆç”¨äºæ€§èƒ½æµ‹è¯•ï¼‰
        test_texts = [doc["text"] for doc in documents[:1000]]
        logger.info(f"âœ“ Test texts prepared: {len(test_texts)} samples")
        
        # 3. ä½¿ç”¨å·²éªŒè¯çš„æ¨¡å‹åˆ—è¡¨
        logger.info(f"\nModels to test: {len(validated_models)}")
        for model in validated_models:
            logger.info(f"  - {model['name']} ({model['dimensions']}ç»´, model_id: {model['model_name']})")
        
        # 4. æ ¹æ®æ¨¡å¼é€‰æ‹©æ‰§è¡Œè·¯å¾„
        if args.async_mode:
            # å¼‚æ­¥æ¨¡å¼
            logger.info("\nğŸš€ Running in ASYNC mode (high performance)")
            asyncio.run(main_async(args, config, logger, validated_models, documents, test_texts))
        else:
            # åŒæ­¥æ¨¡å¼
            logger.info("\nâš™ï¸  Running in SYNC mode (standard)")
            
            report_config = config.get("report", {})
            benchmark = InferenceBenchmark(
                xinference_client=client,
                output_dir=report_config.get("output_dir", "phase1_results")
            )
            
            serial_config = config.get("serial_execution", {})
            cache_config = config.get("vector_cache", {})
            
            logger.info(f"\nStarting serial benchmark...")
            logger.info(f"  Cleanup between models: {serial_config.get('cleanup_between_models', True)}")
            logger.info(f"  Pause between models: {serial_config.get('pause_between_models', 5)}s")
            
            benchmark.run_serial_benchmark(
                models=validated_models,
                test_texts=test_texts,
                documents=documents,
                cache_dir=cache_config.get("output_dir", "vector_cache"),
                cleanup_between_models=serial_config.get("cleanup_between_models", True),
                pause_between_models=serial_config.get("pause_between_models", 5)
            )
            
            # ä¿å­˜ç»“æœ
            logger.info(f"\nSaving final results...")
            benchmark.save_results()
            
            # ç”ŸæˆHTMLæŠ¥å‘Š
            logger.info(f"\nGenerating HTML report...")
            report_generator = Phase1ReportGenerator(
                results_file=str(Path(report_config.get("output_dir", "phase1_results")) / "benchmark_results.json"),
                output_dir=report_config.get("output_dir", "phase1_results")
            )
            report_path = report_generator.generate_report()
            logger.info(f"âœ“ HTML report generated: {report_path}")
            
            # æ‰“å°æ‘˜è¦
            logger.info("\n" + "="*80)
            logger.info("BENCHMARK SUMMARY")
            logger.info("="*80)
            
            summary = benchmark.get_summary()
            for model_summary in summary["models"]:
                logger.info(f"\n{model_summary['name']}:")
                logger.info(f"  Throughput: {model_summary['throughput_docs_per_sec']:.2f} docs/s")
                logger.info(f"  Single latency (P99): {model_summary['single_latency_p99_ms']:.2f} ms")
                logger.info(f"  Optimal batch size: {model_summary['optimal_batch_size']}")
                logger.info(f"  GPU peak memory: {model_summary['gpu_peak_memory_mb']:.2f} MB")
                logger.info(f"  Time for 3M vectors: {model_summary['time_for_3m_vectors_hours']:.2f} hours")
                logger.info(f"  Time for 100M vectors (estimated): {model_summary['time_for_100m_vectors_hours']:.1f} hours")
        
        logger.info("\n" + "="*80)
        logger.info("âœ“ Phase 1 completed successfully!")
        logger.info(f"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("="*80)
        
        return 0
        
    except Exception as e:
        logger.error(f"\nâœ— Phase 1 failed: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())
