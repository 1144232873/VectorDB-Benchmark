#!/usr/bin/env python3
"""
Phase 1: å‘é‡ç”Ÿæˆæ€§èƒ½æµ‹è¯• - ä¸»ç¨‹åº

ä¸²è¡Œæµ‹è¯•4ä¸ªåµŒå…¥æ¨¡å‹çš„æ¨ç†æ€§èƒ½å¹¶ç”Ÿæˆ300ä¸‡å‘é‡ç¼“å­˜
"""

import argparse
import logging
import sys
import yaml
from pathlib import Path
from datetime import datetime

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from phase1_embedding.models.xinference_client import XinferenceClient
from phase1_embedding.data.dataset_loader import DatasetLoader
from phase1_embedding.benchmarks.inference_benchmark import InferenceBenchmark
from phase1_embedding.report_generator import Phase1ReportGenerator


def setup_logging(log_dir: str, log_file: str, console_level: str = "INFO"):
    """
    é…ç½®æ—¥å¿—
    
    Args:
        log_dir: æ—¥å¿—ç›®å½•
        log_file: æ—¥å¿—æ–‡ä»¶å
        console_level: æ§åˆ¶å°æ—¥å¿—çº§åˆ«
    """
    log_dir_path = Path(log_dir)
    log_dir_path.mkdir(parents=True, exist_ok=True)
    
    # æ–‡ä»¶æ—¥å¿—
    log_path = log_dir_path / log_file
    file_handler = logging.FileHandler(log_path, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(file_formatter)
    
    # æ§åˆ¶å°æ—¥å¿—
    console_handler = logging.StreamHandler()
    console_handler.setLevel(getattr(logging, console_level.upper()))
    console_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )
    console_handler.setFormatter(console_formatter)
    
    # é…ç½®root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    
    logging.info(f"Logging initialized: {log_path}")


def load_config(config_file: str) -> dict:
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        é…ç½®å­—å…¸
    """
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Phase 1: å‘é‡ç”Ÿæˆæ€§èƒ½æµ‹è¯•"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="../config/phase1_config.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--serial",
        action="store_true",
        default=True,
        help="ä¸²è¡Œæ‰§è¡Œï¼ˆå¼ºåˆ¶ï¼‰"
    )
    parser.add_argument(
        "--models",
        type=str,
        nargs="+",
        help="æŒ‡å®šè¦æµ‹è¯•çš„æ¨¡å‹ï¼ˆé»˜è®¤æµ‹è¯•æ‰€æœ‰ï¼‰"
    )
    parser.add_argument(
        "--batch",
        type=int,
        help="æŒ‡å®šè¦è¿è¡Œçš„æ‰¹æ¬¡IDï¼ˆå¦‚æœé…ç½®æ–‡ä»¶ä¸­å®šä¹‰äº†batch_groupsï¼‰"
    )
    parser.add_argument(
        "--list-batches",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ‰¹æ¬¡å¹¶é€€å‡º"
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    print(f"Loading config from {args.config}")
    config = load_config(args.config)
    
    # è®¾ç½®æ—¥å¿—
    logging_config = config.get("logging", {})
    setup_logging(
        log_dir=logging_config.get("log_dir", "logs"),
        log_file=logging_config.get("log_file", "phase1.log"),
        console_level=logging_config.get("level", "INFO")
    )
    
    logger = logging.getLogger(__name__)
    
    logger.info("="*80)
    logger.info("Phase 1: å‘é‡ç”Ÿæˆæ€§èƒ½æµ‹è¯•")
    logger.info("="*80)
    logger.info(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # å¦‚æœæŒ‡å®šäº† --list-batchesï¼Œåˆ—å‡ºæ‰€æœ‰æ‰¹æ¬¡å¹¶é€€å‡ºï¼ˆä¸éœ€è¦è¿æ¥Xinferenceï¼‰
    if args.list_batches:
        batch_groups = config.get("batch_groups", [])
        if not batch_groups:
            print("No batch_groups defined in config file")
            return 0
        
        print("\nAvailable batches:")
        for batch in batch_groups:
            batch_id = batch.get("batch_id", "?")
            batch_name = batch.get("batch_name", "unnamed")
            model_names = batch.get("model_names", [])
            print(f"\n  Batch {batch_id}: {batch_name}")
            print(f"    Models: {', '.join(model_names)}")
            print(f"    Run with: python run_phase1.py --config {args.config} --batch {batch_id}")
        return 0
    
    try:
        # 1. åˆå§‹åŒ–Xinferenceå®¢æˆ·ç«¯
        xinference_config = config["xinference"]
        logger.info(f"\nConnecting to Xinference at {xinference_config['host']}:{xinference_config['port']}")
        
        client = XinferenceClient(
            host=xinference_config["host"],
            port=xinference_config["port"],
            timeout=xinference_config.get("timeout", 300)
        )
        
        if not client.check_health():
            raise RuntimeError("Xinference service is not available")
        
        logger.info("âœ“ Xinference client connected")
        
        # 1.5. å¤„ç†æ‰¹æ¬¡é…ç½®
        all_models = config["models"]
        models_to_test = all_models
        
        # å¦‚æœæŒ‡å®šäº†æ‰¹æ¬¡ï¼Œåˆ™è¿‡æ»¤æ¨¡å‹
        if args.batch is not None:
            batch_groups = config.get("batch_groups", [])
            if not batch_groups:
                logger.error("--batch specified but no batch_groups defined in config file")
                return 1
            
            # æŸ¥æ‰¾æŒ‡å®šçš„æ‰¹æ¬¡
            selected_batch = None
            for batch in batch_groups:
                if batch.get("batch_id") == args.batch:
                    selected_batch = batch
                    break
            
            if not selected_batch:
                logger.error(f"Batch {args.batch} not found in config. Available batches:")
                for batch in batch_groups:
                    logger.error(f"  Batch {batch.get('batch_id')}: {batch.get('batch_name', 'unnamed')}")
                return 1
            
            batch_model_names = set(selected_batch.get("model_names", []))
            models_to_test = [m for m in all_models if m["name"] in batch_model_names]
            
            if not models_to_test:
                logger.error(f"No models found in batch {args.batch}")
                return 1
            
            logger.info(f"\nğŸ“¦ Running batch {args.batch}: {selected_batch.get('batch_name', 'unnamed')}")
            logger.info(f"  Models in this batch: {', '.join([m['name'] for m in models_to_test])}")
        
        # å¦‚æœæŒ‡å®šäº† --modelsï¼Œåˆ™è¿›ä¸€æ­¥è¿‡æ»¤
        if args.models:
            models_to_test = [m for m in models_to_test if m["name"] in args.models]
            if not models_to_test:
                logger.error(f"No matching models found: {args.models}")
                return 1
        
        # 1.6. éªŒè¯æ‰€æœ‰æ¨¡å‹æ˜¯å¦å­˜åœ¨
        logger.info("\nValidating models...")
        
        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        available_models = client.get_available_model_ids()
        logger.info(f"Available models on Xinference ({len(available_models)}):")
        for m in available_models[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            logger.info(f"  - {m}")
        if len(available_models) > 10:
            logger.info(f"  ... and {len(available_models) - 10} more")
        
        # éªŒè¯æ¯ä¸ªæ¨¡å‹
        validated_models = []
        for model_config in models_to_test:
            model_name = model_config["name"]
            model_full_name = model_config["model_name"]
            
            exists, actual_id = client.check_model_exists(model_full_name)
            if not exists:
                logger.error(
                    f"\nâœ— Model '{model_full_name}' (config name: '{model_name}') not found!\n"
                    f"  Please ensure the model is loaded in Xinference.\n"
                    f"  You can check available models with: curl http://{xinference_config['host']}:{xinference_config['port']}/v1/models"
                )
                # ç»§ç»­éªŒè¯å…¶ä»–æ¨¡å‹ï¼Œä½†è®°å½•é”™è¯¯
            else:
                if actual_id and actual_id != model_full_name:
                    logger.warning(
                        f"âš  Model name mismatch for '{model_name}':\n"
                        f"  Config uses: '{model_full_name}'\n"
                        f"  Xinference has: '{actual_id}'\n"
                        f"  Will use: '{actual_id}'"
                    )
                    # æ›´æ–°é…ç½®ä¸­çš„æ¨¡å‹åç§°
                    model_config["model_name"] = actual_id
                validated_models.append(model_config)
                logger.info(f"âœ“ Model '{model_name}' validated: {actual_id or model_full_name}")
        
        if not validated_models:
            logger.error("\nâœ— No valid models found! Please check your configuration and Xinference setup.")
            return 1
        
        logger.info(f"\nâœ“ Validated {len(validated_models)}/{len(models_to_test)} models")
        
        # 2. å‡†å¤‡æ•°æ®é›†
        dataset_config = config["dataset"]
        logger.info(f"\nPreparing dataset: {dataset_config['name']}")
        logger.info(f"  Target size: {dataset_config['sample_size']} documents")
        
        loader = DatasetLoader(data_dir=dataset_config["path"])
        
        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
        if not loader.check_dataset():
            raise RuntimeError(
                "Dataset not found. Please prepare dataset first:\n"
                "  cd datasets/scripts && ./quick_start.sh 100000"
            )
        
        # é‡‡æ ·æ–‡æ¡£
        documents = loader.sample_documents(
            num_samples=dataset_config["sample_size"],
            seed=dataset_config.get("seed", 42)
        )
        
        logger.info(f"âœ“ Dataset prepared: {len(documents)} documents")
        
        # å‡†å¤‡æµ‹è¯•æ–‡æœ¬ï¼ˆç”¨äºæ€§èƒ½æµ‹è¯•ï¼‰
        test_texts = [doc["text"] for doc in documents[:1000]]
        logger.info(f"âœ“ Test texts prepared: {len(test_texts)} samples")
        
        # 3. åˆå§‹åŒ–åŸºå‡†æµ‹è¯•
        report_config = config.get("report", {})
        benchmark = InferenceBenchmark(
            xinference_client=client,
            output_dir=report_config.get("output_dir", "phase1_results")
        )
        
        # 4. ä½¿ç”¨å·²éªŒè¯çš„æ¨¡å‹åˆ—è¡¨ï¼ˆå·²åœ¨å‰é¢éªŒè¯ï¼‰
        logger.info(f"\nModels to test: {len(validated_models)}")
        for model in validated_models:
            logger.info(f"  - {model['name']} ({model['dimensions']}ç»´, model_id: {model['model_name']})")
        
        # 5. è¿è¡Œä¸²è¡ŒåŸºå‡†æµ‹è¯•
        serial_config = config.get("serial_execution", {})
        cache_config = config.get("vector_cache", {})
        
        logger.info(f"\nStarting serial benchmark...")
        logger.info(f"  Cleanup between models: {serial_config.get('cleanup_between_models', True)}")
        logger.info(f"  Pause between models: {serial_config.get('pause_between_models', 5)}s")
        
        benchmark.run_serial_benchmark(
            models=validated_models,
            test_texts=test_texts,
            documents=documents,
            cache_dir=cache_config.get("output_dir", "vector_cache"),
            cleanup_between_models=serial_config.get("cleanup_between_models", True),
            pause_between_models=serial_config.get("pause_between_models", 5)
        )
        
        # 6. ä¿å­˜æœ€ç»ˆç»“æœ
        logger.info(f"\nSaving final results...")
        benchmark.save_results()
        
        # 7. ç”ŸæˆHTMLæŠ¥å‘Š
        logger.info(f"\nGenerating HTML report...")
        report_generator = Phase1ReportGenerator(
            results_file=str(Path(report_config.get("output_dir", "phase1_results")) / "benchmark_results.json"),
            output_dir=report_config.get("output_dir", "phase1_results")
        )
        report_path = report_generator.generate_report()
        logger.info(f"âœ“ HTML report generated: {report_path}")
        
        # 8. æ‰“å°æ‘˜è¦
        logger.info("\n" + "="*80)
        logger.info("BENCHMARK SUMMARY")
        logger.info("="*80)
        
        summary = benchmark.get_summary()
        for model_summary in summary["models"]:
            logger.info(f"\n{model_summary['name']}:")
            logger.info(f"  Throughput: {model_summary['throughput_docs_per_sec']:.2f} docs/s")
            logger.info(f"  Single latency (P99): {model_summary['single_latency_p99_ms']:.2f} ms")
            logger.info(f"  Optimal batch size: {model_summary['optimal_batch_size']}")
            logger.info(f"  GPU peak memory: {model_summary['gpu_peak_memory_mb']:.2f} MB")
            logger.info(f"  Time for 3M vectors: {model_summary['time_for_3m_vectors_hours']:.2f} hours")
            logger.info(f"  Time for 100M vectors (estimated): {model_summary['time_for_100m_vectors_hours']:.1f} hours")
        
        logger.info("\n" + "="*80)
        logger.info("âœ“ Phase 1 completed successfully!")
        logger.info(f"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("="*80)
        
        return 0
        
    except Exception as e:
        logger.error(f"\nâœ— Phase 1 failed: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())
