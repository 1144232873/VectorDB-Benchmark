#!/usr/bin/env python3
"""
Phase 1: å‘é‡ç”Ÿæˆæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆå¼‚æ­¥é«˜æ€§èƒ½æ¨¡å¼ï¼‰

ä½¿ç”¨å¼‚æ­¥å¹¶å‘ï¼Œæ¦¨å¹²GPUæ€§èƒ½ï¼Œç”Ÿæˆ300ä¸‡å‘é‡ç¼“å­˜
"""

import argparse
import asyncio
import logging
import sys
import yaml
import httpx
from pathlib import Path
from datetime import datetime

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from phase1_embedding.models.async_xinference_client import AsyncXinferenceClient
from phase1_embedding.data.dataset_loader import DatasetLoader
from phase1_embedding.benchmarks.async_inference_benchmark import AsyncInferenceBenchmark

# è®¾ç½® httpx æ—¥å¿—çº§åˆ«ä¸º WARNINGï¼Œå‡å°‘åˆ·å±
logging.getLogger("httpx").setLevel(logging.WARNING)


def setup_logging(log_dir: str, log_file: str, console_level: str = "WARNING"):
    """é…ç½®æ—¥å¿—"""
    log_dir_path = Path(log_dir)
    log_dir_path.mkdir(parents=True, exist_ok=True)
    
    # æ–‡ä»¶æ—¥å¿—
    log_path = log_dir_path / log_file
    file_handler = logging.FileHandler(log_path, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(file_formatter)
    
    # æ§åˆ¶å°æ—¥å¿—
    console_handler = logging.StreamHandler()
    console_handler.setLevel(getattr(logging, console_level.upper()))
    console_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )
    console_handler.setFormatter(console_formatter)
    
    # é…ç½®root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)


def load_config(config_file: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


async def run_benchmark(config, logger, validated_models, documents, test_texts):
    """è¿è¡Œå¼‚æ­¥åŸºå‡†æµ‹è¯•"""
    xinference_config = config["xinference"]
    perf_config = config.get("performance", {})
    report_config = config.get("report", {})
    cache_config = config.get("vector_cache", {})
    
    # ä½¿ç”¨æé™æ€§èƒ½é…ç½®
    concurrent_requests = perf_config.get("concurrent_requests", 16)
    connection_pool_size = perf_config.get("connection_pool_size", 32)
    
    logger.info(f"\nåˆå§‹åŒ–å¼‚æ­¥å®¢æˆ·ç«¯ï¼ˆæé™æ€§èƒ½æ¨¡å¼ï¼‰...")
    logger.info(f"  å¹¶å‘è¯·æ±‚æ•°: {concurrent_requests}")
    logger.info(f"  è¿æ¥æ± å¤§å°: {connection_pool_size}")
    
    async with AsyncXinferenceClient(
        host=xinference_config["host"],
        port=xinference_config["port"],
        timeout=xinference_config.get("timeout", 300),
        max_concurrent_requests=concurrent_requests,
        connection_pool_size=connection_pool_size
    ) as async_client:
        
        if not await async_client.check_health():
            raise RuntimeError("Xinference æœåŠ¡ä¸å¯ç”¨")
        
        logger.info("âœ“ å¼‚æ­¥å®¢æˆ·ç«¯å·²è¿æ¥")
        
        # åˆå§‹åŒ–åŸºå‡†æµ‹è¯•
        benchmark = AsyncInferenceBenchmark(
            async_client=async_client,
            output_dir=report_config.get("output_dir", "results")
        )
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        logger.info(f"\nå¼€å§‹åŸºå‡†æµ‹è¯•...")
        logger.info(f"  è‡ªåŠ¨æ‰¹æ¬¡è°ƒä¼˜: {perf_config.get('auto_batch_tuning', True)}")
        logger.info(f"  æ¨¡å‹é—´æš‚åœ: {perf_config.get('pause_between_models', 5)}s")
        
        await benchmark.run_serial_benchmark_async(
            models=validated_models,
            test_texts=test_texts,
            documents=documents,
            cache_dir=cache_config.get("output_dir", "results/cache"),
            auto_tune_batch_size=perf_config.get("auto_batch_tuning", True),
            pause_between_models=perf_config.get("pause_between_models", 5)
        )
        
        # ä¿å­˜ç»“æœ
        logger.info(f"\nä¿å­˜ç»“æœ...")
        benchmark.save_results()
        
        # æ‰“å°æ‘˜è¦
        logger.info("\n" + "="*80)
        logger.info("åŸºå‡†æµ‹è¯•æ‘˜è¦")
        logger.info("="*80)
        
        summary = benchmark.get_summary()
        for model_summary in summary["models"]:
            logger.info(f"\n{model_summary['name']}:")
            logger.info(f"  ååé‡: {model_summary['throughput_docs_per_sec']:.2f} docs/s")
            logger.info(f"  æœ€ä¼˜æ‰¹æ¬¡: {model_summary['optimal_batch_size']}")
            logger.info(f"  å¹¶å‘æ•°: {model_summary['concurrent_requests']}")
            logger.info(f"  GPUå³°å€¼: {model_summary['gpu_peak_memory_mb']:.2f} MB")
            logger.info(f"  300ä¸‡å‘é‡è€—æ—¶: {model_summary['time_for_3m_vectors_hours']:.2f} å°æ—¶")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Phase 1: å‘é‡ç”Ÿæˆæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆå¼‚æ­¥é«˜æ€§èƒ½ï¼‰"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="../config/phase1_config.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--models",
        type=str,
        nargs="+",
        help="æŒ‡å®šè¦æµ‹è¯•çš„æ¨¡å‹ï¼ˆé»˜è®¤æµ‹è¯•æ‰€æœ‰ï¼‰"
    )
    parser.add_argument(
        "--batch",
        type=int,
        help="æŒ‡å®šæ‰¹æ¬¡IDï¼ˆç”¨äºæ˜¾å­˜ä¸è¶³æ—¶åˆ†æ‰¹æµ‹è¯•ï¼‰"
    )
    parser.add_argument(
        "--list-batches",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ‰¹æ¬¡"
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    print(f"åŠ è½½é…ç½®: {args.config}")
    config = load_config(args.config)
    
    # è®¾ç½®æ—¥å¿—
    logging_config = config.get("logging", {})
    setup_logging(
        log_dir=logging_config.get("log_dir", "logs"),
        log_file=logging_config.get("log_file", "phase1.log"),
        console_level=logging_config.get("level", "WARNING")
    )
    
    logger = logging.getLogger(__name__)
    
    logger.info("="*80)
    logger.info("Phase 1: å‘é‡ç”Ÿæˆæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆå¼‚æ­¥æé™æ€§èƒ½ï¼‰")
    logger.info("="*80)
    logger.info(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # åˆ—å‡ºæ‰¹æ¬¡
    if args.list_batches:
        batch_groups = config.get("batch_groups", [])
        if not batch_groups:
            print("é…ç½®æ–‡ä»¶ä¸­æœªå®šä¹‰æ‰¹æ¬¡")
            return 0
        
        print("\nå¯ç”¨æ‰¹æ¬¡:")
        for batch in batch_groups:
            batch_id = batch.get("batch_id", "?")
            batch_name = batch.get("batch_name", "unnamed")
            model_names = batch.get("model_names", [])
            print(f"\n  æ‰¹æ¬¡ {batch_id}: {batch_name}")
            print(f"    æ¨¡å‹: {', '.join(model_names)}")
            print(f"    è¿è¡Œ: python benchmark.py --batch {batch_id}")
        return 0
    
    try:
        # æ£€æŸ¥ä¾èµ–
        try:
            import httpx
        except ImportError:
            logger.error("éœ€è¦ httpx åŒ…ï¼Œå®‰è£…: uv add 'httpx[http2]'")
            return 1
        
        # åˆå§‹åŒ–å¼‚æ­¥å®¢æˆ·ç«¯è¿›è¡Œæ¨¡å‹éªŒè¯
        xinference_config = config["xinference"]
        logger.info(f"\nè¿æ¥åˆ° Xinference: {xinference_config['host']}:{xinference_config['port']}")
        
        async with AsyncXinferenceClient(
            host=xinference_config["host"],
            port=xinference_config["port"],
            timeout=xinference_config.get("timeout", 300)
        ) as client:
            
            if not await client.check_health():
                raise RuntimeError("Xinference æœåŠ¡ä¸å¯ç”¨")
            
            logger.info("âœ“ Xinference å·²è¿æ¥")
            
            # å¤„ç†æ‰¹æ¬¡é…ç½®
            all_models = config["models"]
            models_to_test = all_models
            
            if args.batch is not None:
                batch_groups = config.get("batch_groups", [])
                if not batch_groups:
                    logger.error("æŒ‡å®šäº† --batch ä½†é…ç½®æ–‡ä»¶ä¸­æœªå®šä¹‰ batch_groups")
                    return 1
                
                selected_batch = None
                for batch in batch_groups:
                    if batch.get("batch_id") == args.batch:
                        selected_batch = batch
                        break
                
                if not selected_batch:
                    logger.error(f"æœªæ‰¾åˆ°æ‰¹æ¬¡ {args.batch}")
                    return 1
                
                batch_model_names = set(selected_batch.get("model_names", []))
                models_to_test = [m for m in all_models if m["name"] in batch_model_names]
                
                logger.info(f"\nè¿è¡Œæ‰¹æ¬¡ {args.batch}: {selected_batch.get('batch_name', 'unnamed')}")
                logger.info(f"  æ¨¡å‹: {', '.join([m['name'] for m in models_to_test])}")
            
            # æŒ‡å®šæ¨¡å‹è¿‡æ»¤
            if args.models:
                models_to_test = [m for m in models_to_test if m["name"] in args.models]
                if not models_to_test:
                    logger.error(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ¨¡å‹: {args.models}")
                    return 1
            
            # éªŒè¯æ¨¡å‹
            logger.info("\néªŒè¯æ¨¡å‹...")
            available_models = await client.list_models()
            available_model_ids = [m.get("id", m.get("model_uid")) for m in available_models]
            
            logger.info(f"Xinference ä¸Šå¯ç”¨æ¨¡å‹æ•°: {len(available_model_ids)}")
            
            validated_models = []
            for model_config in models_to_test:
                model_name = model_config["name"]
                model_full_name = model_config["model_name"]
                
                # ç®€å•æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨åˆ—è¡¨ä¸­
                if model_full_name in available_model_ids or any(model_full_name in m for m in available_model_ids):
                    validated_models.append(model_config)
                    logger.info(f"âœ“ æ¨¡å‹ '{model_name}' å·²éªŒè¯")
                else:
                    logger.warning(f"âš  æ¨¡å‹ '{model_name}' ({model_full_name}) æœªæ‰¾åˆ°")
            
            if not validated_models:
                logger.error("æœªæ‰¾åˆ°æœ‰æ•ˆæ¨¡å‹")
                return 1
            
            logger.info(f"\nâœ“ å·²éªŒè¯ {len(validated_models)}/{len(models_to_test)} ä¸ªæ¨¡å‹")
            
            # å‡†å¤‡æ•°æ®é›†
            dataset_config = config["dataset"]
            logger.info(f"\nå‡†å¤‡æ•°æ®é›†: {dataset_config['name']}")
            logger.info(f"  ç›®æ ‡å¤§å°: {dataset_config['sample_size']} æ–‡æ¡£")
            
            loader = DatasetLoader(data_dir=dataset_config["path"])
            
            if not loader.check_dataset():
                raise RuntimeError(
                    "æ•°æ®é›†æœªæ‰¾åˆ°ï¼Œè¯·å…ˆå‡†å¤‡æ•°æ®é›†:\n"
                    "  cd datasets/scripts && ./quick_start.sh 100000"
                )
            
            # é‡‡æ ·æ–‡æ¡£
            documents = loader.sample_documents(
                num_samples=dataset_config["sample_size"],
                seed=dataset_config.get("seed", 42)
            )
            
            logger.info(f"âœ“ æ•°æ®é›†å·²å‡†å¤‡: {len(documents)} æ–‡æ¡£")
            
            # å‡†å¤‡æµ‹è¯•æ–‡æœ¬
            test_texts = [doc["text"] for doc in documents[:1000]]
            logger.info(f"âœ“ æµ‹è¯•æ–‡æœ¬å·²å‡†å¤‡: {len(test_texts)} æ ·æœ¬")
            
            # æ˜¾ç¤ºæµ‹è¯•æ¨¡å‹
            logger.info(f"\nå¾…æµ‹è¯•æ¨¡å‹: {len(validated_models)}")
            for model in validated_models:
                logger.info(f"  - {model['name']} ({model['dimensions']}ç»´)")
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        logger.info("\nğŸš€ å¯åŠ¨å¼‚æ­¥æé™æ€§èƒ½æµ‹è¯•")
        await run_benchmark(config, logger, validated_models, documents, test_texts)
        
        logger.info("\n" + "="*80)
        logger.info("âœ“ Phase 1 å®Œæˆ")
        logger.info(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("="*80)
        
        return 0
        
    except Exception as e:
        logger.error(f"\nâœ— Phase 1 å¤±è´¥: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))
